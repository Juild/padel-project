{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "    def __init__(self, base_model, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(base_model.fc.in_features, 128), # don't really understand what is happening here with base_model.fc.in_features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, self.numClasses)\n",
    "\t\t)\n",
    "\t\t# set the classifier of our base model to produce outputs\n",
    "\t\t# from the last convolution block\n",
    "        self.base_model.fc = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        bboxes = self.regressor(x)\n",
    "        class_logits = self.classifier(x)\n",
    "        return (bboxes, class_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_path, images_path, transforms=None) -> None:\n",
    "        self.transforms = transforms\n",
    "        self.images: List[torch.Tensor] = []\n",
    "        self.image_bboxes: List[Dict] = []\n",
    "        \n",
    "        annotations_files = os.listdir(annotations_path)\n",
    "        for json_file_name in annotations_files:\n",
    "            file_path = annotations_path + json_file_name\n",
    "            f  = open(file_path, 'r')\n",
    "            annotation: Dict = json.load(f)[0] # as it is a list with one element the dict\n",
    "            coordinates: Dict = {\n",
    "                \"boxes\": []\n",
    "            }\n",
    "            \n",
    "            for bbox in annotation[\"annotations\"]:\n",
    "                coordinates[\"boxes\"].append(bbox[\"coordinates\"])\n",
    "\n",
    "            self.image_bboxes.append(coordinates)\n",
    "            image = cv2.imread(images_path + annotation[\"image\"])\n",
    "            image: torch.Tensor = torch.tensor(image).permute(1,0,2) # (X, Y, RGB) (W,H,RGB) we do this to match with the bboxes coordinates (x,y)\n",
    "            self.images.append(image)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.transforms:\n",
    "            return (self.transforms(self.images[idx]) , self.image_bboxes[idx])\n",
    "        else:\n",
    "            return (self.images[idx], self.image_bboxes[idx])\n",
    "    def __len__(self):\n",
    "        return self.images.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[180, 145, 102],\n",
       "          [255, 224, 181],\n",
       "          [252, 218, 175],\n",
       "          ...,\n",
       "          [ 64,  30, 225],\n",
       "          [ 68,  38, 221],\n",
       "          [ 54,  26, 203]],\n",
       " \n",
       "         [[122,  83,  44],\n",
       "          [255, 233, 193],\n",
       "          [255, 234, 194],\n",
       "          ...,\n",
       "          [ 64,  30, 225],\n",
       "          [ 68,  38, 221],\n",
       "          [ 54,  26, 203]],\n",
       " \n",
       "         [[ 88,  43,   9],\n",
       "          [190, 148, 113],\n",
       "          [255, 225, 187],\n",
       "          ...,\n",
       "          [ 64,  30, 225],\n",
       "          [ 68,  38, 221],\n",
       "          [ 54,  26, 203]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[195, 155, 126],\n",
       "          [248, 206, 177],\n",
       "          [216, 172, 143],\n",
       "          ...,\n",
       "          [ 66,  32, 222],\n",
       "          [ 68,  38, 217],\n",
       "          [ 54,  26, 199]],\n",
       " \n",
       "         [[205, 167, 137],\n",
       "          [241, 201, 172],\n",
       "          [176, 134, 105],\n",
       "          ...,\n",
       "          [ 65,  31, 221],\n",
       "          [ 66,  36, 215],\n",
       "          [ 53,  25, 198]],\n",
       " \n",
       "         [[215, 179, 149],\n",
       "          [239, 201, 171],\n",
       "          [147, 107,  78],\n",
       "          ...,\n",
       "          [ 66,  33, 221],\n",
       "          [ 67,  38, 215],\n",
       "          [ 54,  26, 199]]], dtype=torch.uint8),\n",
       " {'boxes': [{'x': 550.4253731343284,\n",
       "    'y': 437.5,\n",
       "    'width': 21.0,\n",
       "    'height': 17.0},\n",
       "   {'x': 1366.9253731343285, 'y': 637.0, 'width': 16.0, 'height': 14.0}]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ImageDataset(annotations_path=\"../../datasets/annotations/\", images_path=\"../../../frames/\")\n",
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

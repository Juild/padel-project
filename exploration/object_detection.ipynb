{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images: Dict, transforms=None) -> None:\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, index):\n",
    "        image = self.tensors[0][index]\n",
    "        label = self.tensors[1][index]\n",
    "        bbox = self.tensors[2][index]\n",
    "        \n",
    "        image = image.permute(2, 0, 1)\n",
    "        if self.transforms_\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return (image, label, bbox)\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "    def __init__(self, base_model, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(base_model.fc.in_features, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, self.numClasses)\n",
    "\t\t)\n",
    "\t\t# set the classifier of our base model to produce outputs\n",
    "\t\t# from the last convolution block\n",
    "        self.base_model.fc = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        bboxes = self.regressor(x)\n",
    "        class_logits = self.classifier(x)\n",
    "        return (bboxes, class_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# training script\n",
    "\n",
    "##  Read the annoatitions from the directory\n",
    "annotations_path = \"../../datasets/annotations/\"\n",
    "images_path = \"../frames/\"\n",
    "annotations_files = os.listdir(\"../../datasets/annotations/\")\n",
    "images: List[torch.Tensor] = []\n",
    "image_bboxes = []\n",
    "for json_file_name in annotations_files:\n",
    "    full_annotations_path = annotations_path + json_file_name\n",
    "    f  = open(full_annotations_path, 'r')\n",
    "    annotation_dict = json.load(f)[0] # as it is a list with one element the dict\n",
    "    coordinates: Dict = {\n",
    "        \"boxes\": []\n",
    "    }\n",
    "    for bbox in annotation_dict[\"annotations\"]:\n",
    "        coordinates[\"boxes\"].append(bbox[\"coordinates\"])\n",
    "        #print(coordinates[\"boxes\"])\n",
    "    image_bboxes.append(coordinates)\n",
    "    #print(image_bboxes[0])\n",
    "    image = cv2.imread(images_path + annotation_dict[\"image\"])\n",
    "    image: torch.Tensor = torch.tensor(image).permute(1,0,2) # (X, Y, RGB) (W,H,RGB) we do this to match with the bboxes coordinates (x,y)\n",
    "    images.append(image)\n",
    "    break\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

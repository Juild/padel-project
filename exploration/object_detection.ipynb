{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "    def __init__(self, base_model, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(base_model.fc.in_features, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, self.numClasses)\n",
    "\t\t)\n",
    "\t\t# set the classifier of our base model to produce outputs\n",
    "\t\t# from the last convolution block\n",
    "        self.base_model.fc = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        bboxes = self.regressor(x)\n",
    "        class_logits = self.classifier(x)\n",
    "        return (bboxes, class_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_path, images_path, transforms=None) -> None:\n",
    "        self.transforms = transforms\n",
    "        self.images: List[torch.Tensor] = []\n",
    "        self.image_bboxes: List[Dict] = []\n",
    "        \n",
    "        annotations_files = os.listdir(annotations_path)\n",
    "        for json_file_name in annotations_files:\n",
    "            file_path = annotations_path + json_file_name\n",
    "            f = open(file_path, 'r')\n",
    "            annotation: Dict = json.load(f)[0] # as it is a list with one element the dict\n",
    "            coordinates: Dict = {\n",
    "                \"boxes\": []\n",
    "            }\n",
    "            \n",
    "            for bbox in annotation[\"annotations\"]:\n",
    "                coordinates[\"boxes\"].append(bbox[\"coordinates\"])\n",
    "\n",
    "            self.image_bboxes.append(coordinates)\n",
    "            image = cv2.imread(images_path + annotation[\"image\"])\n",
    "            image: torch.Tensor = torch.tensor(image, dtype=float).permute(1,0,2) # (X, Y, RGB) (W,H,RGB) we do this to match with the bboxes coordinates (x,y)\n",
    "            self.images.append(image)\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        for image_tensor in self.images:\n",
    "            for channel in range(image_tensor.shape[2]):\n",
    "                self.means.append(image_tensor[:, :, channel].reshape(image_tensor.shape[0] * image_tensor.shape[1]).mean(dtype=float))\n",
    "                self.stds.append(image_tensor[:, :, channel].reshape(image_tensor.shape[0] * image_tensor.shape[1]).std())\n",
    "        \n",
    "    def __getitem__(self, idx): \n",
    "        if self.transforms:\n",
    "            return (self.transforms(self.images[idx]) , self.image_bboxes[idx])\n",
    "        else:\n",
    "            return (self.images[idx], self.image_bboxes[idx])\n",
    "    def __len__(self):\n",
    "        return self.images.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(160.4634, dtype=torch.float64),\n",
       " tensor(87.7588, dtype=torch.float64),\n",
       " tensor(103.3183, dtype=torch.float64),\n",
       " tensor(160.8827, dtype=torch.float64),\n",
       " tensor(87.8500, dtype=torch.float64),\n",
       " tensor(103.3913, dtype=torch.float64),\n",
       " tensor(160.8849, dtype=torch.float64),\n",
       " tensor(87.9408, dtype=torch.float64),\n",
       " tensor(103.4789, dtype=torch.float64),\n",
       " tensor(161.1748, dtype=torch.float64),\n",
       " tensor(88.1728, dtype=torch.float64),\n",
       " tensor(103.7497, dtype=torch.float64),\n",
       " tensor(160.7850, dtype=torch.float64),\n",
       " tensor(86.2934, dtype=torch.float64),\n",
       " tensor(106.5704, dtype=torch.float64),\n",
       " tensor(160.7273, dtype=torch.float64),\n",
       " tensor(86.2996, dtype=torch.float64),\n",
       " tensor(106.5258, dtype=torch.float64),\n",
       " tensor(160.7880, dtype=torch.float64),\n",
       " tensor(86.3078, dtype=torch.float64),\n",
       " tensor(106.5508, dtype=torch.float64),\n",
       " tensor(160.5984, dtype=torch.float64),\n",
       " tensor(86.3099, dtype=torch.float64),\n",
       " tensor(106.3697, dtype=torch.float64),\n",
       " tensor(160.9675, dtype=torch.float64),\n",
       " tensor(86.3305, dtype=torch.float64),\n",
       " tensor(106.7313, dtype=torch.float64),\n",
       " tensor(160.8847, dtype=torch.float64),\n",
       " tensor(86.3401, dtype=torch.float64),\n",
       " tensor(106.6332, dtype=torch.float64),\n",
       " tensor(160.9895, dtype=torch.float64),\n",
       " tensor(86.3482, dtype=torch.float64),\n",
       " tensor(106.7430, dtype=torch.float64),\n",
       " tensor(160.6186, dtype=torch.float64),\n",
       " tensor(86.3677, dtype=torch.float64),\n",
       " tensor(106.4122, dtype=torch.float64),\n",
       " tensor(161.0123, dtype=torch.float64),\n",
       " tensor(86.2729, dtype=torch.float64),\n",
       " tensor(106.7826, dtype=torch.float64),\n",
       " tensor(160.7642, dtype=torch.float64),\n",
       " tensor(86.2141, dtype=torch.float64),\n",
       " tensor(106.5293, dtype=torch.float64),\n",
       " tensor(160.8228, dtype=torch.float64),\n",
       " tensor(86.1207, dtype=torch.float64),\n",
       " tensor(106.5789, dtype=torch.float64),\n",
       " tensor(160.2759, dtype=torch.float64),\n",
       " tensor(85.7356, dtype=torch.float64),\n",
       " tensor(106.2098, dtype=torch.float64),\n",
       " tensor(159.8625, dtype=torch.float64),\n",
       " tensor(85.5170, dtype=torch.float64),\n",
       " tensor(105.8143, dtype=torch.float64),\n",
       " tensor(161.4033, dtype=torch.float64),\n",
       " tensor(85.7115, dtype=torch.float64),\n",
       " tensor(101.9621, dtype=torch.float64),\n",
       " tensor(161.3695, dtype=torch.float64),\n",
       " tensor(85.7017, dtype=torch.float64),\n",
       " tensor(101.9344, dtype=torch.float64),\n",
       " tensor(161.2489, dtype=torch.float64),\n",
       " tensor(85.6540, dtype=torch.float64),\n",
       " tensor(101.8243, dtype=torch.float64),\n",
       " tensor(161.0358, dtype=torch.float64),\n",
       " tensor(85.3994, dtype=torch.float64),\n",
       " tensor(101.2894, dtype=torch.float64),\n",
       " tensor(160.9196, dtype=torch.float64),\n",
       " tensor(85.3628, dtype=torch.float64),\n",
       " tensor(101.1197, dtype=torch.float64),\n",
       " tensor(160.9071, dtype=torch.float64),\n",
       " tensor(85.2590, dtype=torch.float64),\n",
       " tensor(101.1867, dtype=torch.float64),\n",
       " tensor(160.9953, dtype=torch.float64),\n",
       " tensor(85.2132, dtype=torch.float64),\n",
       " tensor(101.2602, dtype=torch.float64),\n",
       " tensor(161.0614, dtype=torch.float64),\n",
       " tensor(85.2535, dtype=torch.float64),\n",
       " tensor(101.3946, dtype=torch.float64),\n",
       " tensor(160.8736, dtype=torch.float64),\n",
       " tensor(85.2488, dtype=torch.float64),\n",
       " tensor(101.2121, dtype=torch.float64),\n",
       " tensor(160.6814, dtype=torch.float64),\n",
       " tensor(85.3122, dtype=torch.float64),\n",
       " tensor(101.1120, dtype=torch.float64),\n",
       " tensor(160.8153, dtype=torch.float64),\n",
       " tensor(85.3572, dtype=torch.float64),\n",
       " tensor(101.3501, dtype=torch.float64),\n",
       " tensor(160.8007, dtype=torch.float64),\n",
       " tensor(85.3435, dtype=torch.float64),\n",
       " tensor(101.3274, dtype=torch.float64),\n",
       " tensor(162.7883, dtype=torch.float64),\n",
       " tensor(86.7441, dtype=torch.float64),\n",
       " tensor(101.5294, dtype=torch.float64),\n",
       " tensor(163.0085, dtype=torch.float64),\n",
       " tensor(86.7670, dtype=torch.float64),\n",
       " tensor(101.7269, dtype=torch.float64),\n",
       " tensor(162.7809, dtype=torch.float64),\n",
       " tensor(86.7281, dtype=torch.float64),\n",
       " tensor(101.6151, dtype=torch.float64),\n",
       " tensor(162.9871, dtype=torch.float64),\n",
       " tensor(86.8210, dtype=torch.float64),\n",
       " tensor(101.6069, dtype=torch.float64)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ImageDataset(\n",
    "    annotations_path=\"../datasets/annotations/\",\n",
    "    images_path=\"../../frames/\",)\n",
    "dataset.means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector(nn.Module):\n",
    "    def __init__(self, base_model, num_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(base_model.fc.in_features, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(base_model.fc.in_features, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, 512),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Dropout(),\n",
    "\t\t\tnn.Linear(512, self.numClasses)\n",
    "\t\t)\n",
    "\t\t# set the classifier of our base model to produce outputs\n",
    "\t\t# from the last convolution block\n",
    "        self.base_model.fc = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        bboxes = self.regressor(x)\n",
    "        class_logits = self.classifier(x)\n",
    "        return (bboxes, class_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_path, images_path, channels, transforms=None) -> None:\n",
    "        self.transforms = transforms\n",
    "        self.images: List[torch.Tensor] = []\n",
    "        self.image_bboxes: List[Dict] = []\n",
    "        \n",
    "        annotations_files = os.listdir(annotations_path)\n",
    "        for json_file_name in annotations_files:\n",
    "            file_path = annotations_path + json_file_name\n",
    "            f = open(file_path, 'r')\n",
    "            annotation: Dict = json.load(f)[0] # as it is a list with one element the dict\n",
    "            coordinates: Dict = {\n",
    "                \"boxes\": []\n",
    "            }\n",
    "            \n",
    "            for bbox in annotation[\"annotations\"]:\n",
    "                coordinates[\"boxes\"].append(bbox[\"coordinates\"])\n",
    "\n",
    "            self.image_bboxes.append(coordinates)\n",
    "            image = cv2.imread(images_path + annotation[\"image\"])\n",
    "            image: torch.Tensor = torch.tensor(image, dtype=float).permute(1,0,2) # (X, Y, RGB) (W,H,RGB) we do this to match with the bboxes coordinates (x,y)\n",
    "            image = (image * 2)/255. - 1 # normalization [-1, 1]\n",
    "            self.images.append(image)\n",
    "        # Compute the mean of the dataset\n",
    "        stack = torch.stack(tuple(self.images))\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        for channel in range(channels):\n",
    "            channel_stack: List[torch.Tensor] = stack[:, :, :, channel].reshape(stack.shape[0] * stack.shape[1] * stack.shape[2])\n",
    "            self.means.append(\n",
    "                float(channel_stack.mean())\n",
    "                )\n",
    "            self.stds.append(\n",
    "                float(channel_stack.std())\n",
    "                )\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if self.transforms:\n",
    "            return (self.transforms(self.images[idx]) , self.image_bboxes[idx])\n",
    "        else:\n",
    "            return (self.images[idx], self.image_bboxes[idx])\n",
    "    def __len__(self):\n",
    "        return self.images.size(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "dataset = ImageDataset(\n",
    "    annotations_path=\"../datasets/annotations/\",\n",
    "    images_path=\"../../frames/\",\n",
    "    channels=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26350067177310516"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(dataset.means[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2073600]' is invalid for input of size 4147200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-0e6e35cf0ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1920\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1080\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2073600]' is invalid for input of size 4147200"
     ]
    }
   ],
   "source": [
    "torch.stack((dataset[0][0], dataset[1][0]))[:, :, :, 0].reshape(1920 * 1080 ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[[1 ,2, 3 ]], [[3, 4, 4]]])\n",
    "x[:, :, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
